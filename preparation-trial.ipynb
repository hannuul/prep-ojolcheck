{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> IMPORT LIBRARY <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DEFINE AND CREATE FOLDERS <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder paths\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "folder_merge = r'\\_MERGE'\n",
    "merge_path = script_dir+folder_merge\n",
    "if not os.path.exists(merge_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(merge_path)\n",
    "\n",
    "folder_final = r'\\_FINAL'\n",
    "final_path = script_dir+folder_final\n",
    "if not os.path.exists(final_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(final_path)\n",
    "\n",
    "folder_qris = r'\\QRIS'\n",
    "qris_path = script_dir+folder_qris\n",
    "\n",
    "folder_shopee = r'\\SHOPEE'\n",
    "shopee_path = script_dir+folder_shopee\n",
    "\n",
    "folder_grab = r'\\GRAB'\n",
    "grab_path = script_dir+folder_grab\n",
    "\n",
    "folder_gojek1 = r'\\GO_Laporan'\n",
    "gojek1_path = script_dir+folder_gojek1\n",
    "\n",
    "folder_gojek2 = r'\\GO_Mie'\n",
    "gojek2_path = script_dir+folder_gojek2\n",
    "\n",
    "folder_web = r'\\WEB'\n",
    "web_path = script_dir+folder_web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<h1>PREPARING GRAB<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining Grab Invoice Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(grab_path)\n",
    "\n",
    "# Filter out non-CSV files\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "# Create a list to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(grab_path, csv)\n",
    "    try:\n",
    "        # Try reading the file using default UTF-8 encoding\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        df_list.append(df)\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # If UTF-8 fails, try reading the file using UTF-16 encoding with tab separator\n",
    "            df = pd.read_csv(file_path, sep='\\t', encoding='utf-16')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {csv} because of error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_grab = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = merge_path+\"/\"+'merged_GRAB.csv'\n",
    "\n",
    "if os.path.exists(new_file_path):\n",
    "    os.remove(new_file_path)\n",
    "\n",
    "big_grab.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning Combined Grab File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_grab = pd.read_csv(merge_path+'\\merged_grab.csv',sep=',',decimal='.')\n",
    "\n",
    "reduced_grab   =   merged_grab.loc[:,[\"Store Name\",\n",
    "                                      \"Updated On\",\n",
    "                                      \"Status\",\n",
    "                                      \"Short Order ID\",\n",
    "                                      \"Net Sales\",\n",
    "                                      \"Amount\"]].rename(columns={ \"Store Name\"      :\"CAB\",\n",
    "                                                                  \"Updated On\"      :\"DATETIME\",\n",
    "                                                                  \"Status\"          :\"Status\",\n",
    "                                                                  \"Short Order ID\"  :\"ID\",\n",
    "                                                                  \"Net Sales\"       :\"NOM\",\n",
    "                                                                  \"Amount\":\"NOM2\"}).fillna(\"\")\n",
    "\n",
    "\n",
    "reduced_grab['DATETIME']    =   pd.to_datetime(reduced_grab['DATETIME'], format='%d %b %Y %I:%M %p')\n",
    "reduced_grab['DATE']        =   reduced_grab['DATETIME'].dt.strftime('%d/%m/%Y')\n",
    "reduced_grab['TIME']        =   reduced_grab['DATETIME'].dt.time\n",
    "\n",
    "reduced_grab                =  reduced_grab.drop(reduced_grab[reduced_grab['Status'] == 'Cancelled'].index)\n",
    "\n",
    "reduced_grab['CODE']        =   \"\"\n",
    "reduced_grab['KAT']         =   \"GRAB FOOD\"\n",
    "reduced_grab['SOURCE']      =   \"INVOICE\"\n",
    "\n",
    "reduced_grab.loc[reduced_grab['NOM'] == '', 'CODE'] = 'Adjustment'\n",
    "reduced_grab.loc[reduced_grab['NOM'] == '', 'NOM'] = reduced_grab['NOM2']\n",
    "reduced_grab['NOM']         =   pd.to_numeric(reduced_grab['NOM']).astype(int)\n",
    "\n",
    "# re-order columns\n",
    "reduced_grab  =   reduced_grab[[\"CAB\", \"DATE\", \"TIME\", \"CODE\", \"ID\", \"NOM\", \"KAT\", \"SOURCE\"]]\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = final_path+\"/\"+'reduced_GRAB.csv'\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "reduced_grab.to_csv(os.path.join(final_path, 'reduced_GRAB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<h1> PREPARING SHOPEE <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining Shopeefood Invoice Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Shopee\n",
    "all_files = os.listdir(shopee_path)\n",
    "\n",
    "# Filter out non-CSV files\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "# Create a list to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(shopee_path, csv)\n",
    "    try:\n",
    "        # Try reading the file using default UTF-8 encoding\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # If UTF-8 fails, try reading the file using UTF-16 encoding with tab separator\n",
    "            df = pd.read_csv(file_path, sep='\\t', encoding='utf-16')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {csv} because of error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_shopee = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = merge_path+\"/\"+'merged_SHOPEE.csv'\n",
    "\n",
    "if os.path.exists(new_file_path):\n",
    "    os.remove(new_file_path)\n",
    "\n",
    "big_shopee.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning Combined Shopeefood File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_shopee = pd.read_csv(merge_path+'\\merged_SHOPEE.csv',sep=',',decimal='.')\n",
    "\n",
    "reduced_shopee   =   merged_shopee.loc[:,[\"Order Pick up ID\",\n",
    "                                        \"Store Name\",\n",
    "                                        \"Order Complete/Cancel Time\",\n",
    "                                        \"Order Amount\",\n",
    "                                        \"Order Status\"]].rename(columns={   \"Order Pick up ID\"          : \"ID\",\n",
    "                                                                            \"Store Name\"                : \"CAB\",\n",
    "                                                                            \"Order Complete/Cancel Time\": \"DATETIME\",\n",
    "                                                                            \"Order Amount\"              : \"NOM\",\n",
    "                                                                            \"Order Status\"              : \"Status\"}).fillna(\"\")\n",
    "\n",
    "reduced_shopee['DATETIME']    =   pd.to_datetime(reduced_shopee['DATETIME'], format='%d/%m/%Y %H:%M:%S')\n",
    "reduced_shopee['DATE']        =   reduced_shopee['DATETIME'].dt.strftime('%d/%m/%Y')\n",
    "reduced_shopee['TIME']        =   reduced_shopee['DATETIME'].dt.time\n",
    "\n",
    "reduced_shopee['NOM']         =   pd.to_numeric(reduced_shopee['NOM']).astype(int)\n",
    "reduced_shopee                =  reduced_shopee.drop(reduced_shopee[reduced_shopee['Status'] == 'Cancelled'].index)\n",
    "\n",
    "reduced_shopee['CODE']        =   \"\"\n",
    "reduced_shopee['KAT']         =   \"SHOPEEPAY\"\n",
    "reduced_shopee['SOURCE']      =   \"INVOICE\"\n",
    "\n",
    "# re-order columns\n",
    "reduced_shopee                =   reduced_shopee[[\"CAB\", \"DATE\", \"TIME\", \"CODE\", \"ID\", \"NOM\", \"KAT\", \"SOURCE\"]]\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "reduced_shopee.to_csv(os.path.join(final_path, 'reduced_SHOPEE.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<h1>PREPARING QRIS SHOPEE<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining QRIS Shopee Invoice Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine QRIS Shopee\n",
    "all_files = os.listdir(qris_path)\n",
    "\n",
    "#Copy File to a new File\n",
    "csv_files = [f for f in all_files if f.endswith('.csv') and not f.startswith('new ')]\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(qris_path, csv)\n",
    "    try:\n",
    "        # Try reading the file using semicolon delimiter\n",
    "        df = pd.read_csv(file_path, sep=';', escapechar='\"')\n",
    "        df.to_csv(os.path.join(qris_path, 'new ' + csv), sep=',', index=False)\n",
    "        # df_new = pd.read_csv(qris_path + '\\new' + csv)\n",
    "        # df_list.append(df_new)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} and create its csv because: {e}\")\n",
    "\n",
    "# Combine New QRIS Files\n",
    "new_files = glob.glob(os.path.join(qris_path, 'new' + '*.csv'))\n",
    "\n",
    "df_list = []\n",
    "for new in new_files:\n",
    "    # file_path = glob.glob(os.path.join(qris_path, 'new' + '*.csv'))\n",
    "    try:\n",
    "        df = pd.read_csv(new)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {new} and combine it because {e}\")\n",
    "\n",
    "# Concatenate all data with same separation into one dataframe\n",
    "big_qris = pd.concat(df_list, ignore_index=True)\n",
    "big_qris = big_qris.dropna(axis=1, how='all')\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = merge_path+\"/\"+'merged_QRIS.csv'\n",
    "\n",
    "if os.path.exists(new_file_path):\n",
    "    os.remove(new_file_path)\n",
    "big_qris.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning Combined QRIS Shopee File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_qris = pd.read_csv(merge_path+'\\merged_QRIS.csv', quotechar=None, delimiter=',', quoting=3, escapechar='\"', decimal='.')\n",
    "merged_qris.replace(0, np.nan, inplace=True)\n",
    "merged_qris = merged_qris.dropna(axis=1, how='all')\n",
    "\n",
    "#Fix Date/Time with Update Time\n",
    "merged_qris.loc[merged_qris['Merchant Scope'] == '1', 'Create Time'] = np.nan\n",
    "merged_qris['Create Time'] = merged_qris['Create Time'].astype('datetime64[ns]')\n",
    "merged_qris['Terminal ID'] = merged_qris['Terminal ID'].astype('datetime64[ns]')\n",
    "merged_qris.loc[merged_qris['Merchant Scope'] == '1', 'Create Time'] = merged_qris['Terminal ID']\n",
    "\n",
    "#Fix Transaction Amount\n",
    "merged_qris['Issuer Identifier'] = merged_qris['Issuer Identifier'].astype(object, errors='ignore')\n",
    "merged_qris.loc[merged_qris['Merchant Scope'] == '1', 'Issuer Identifier'] = merged_qris['External Reference ID']\n",
    "merged_qris['Issuer Identifier'] = merged_qris['Issuer Identifier'].astype('Float64')\n",
    "merged_qris['Issuer Identifier'] = merged_qris['Issuer Identifier'].astype('int64')\n",
    "\n",
    "#Fix Transaction ID\n",
    "merged_qris.loc[merged_qris['Merchant Scope'] == '1', 'Merchant Scope'] = '1' + merged_qris['Transaction ID'].str[:-2] \n",
    "\n",
    "reduced_qris    =   merged_qris.loc[:,[\"Partner Merchant ID\",\n",
    "                                    \"Create Time\",\n",
    "                                    \"Merchant Scope\",\n",
    "                                    \"Issuer Identifier\",\n",
    "                                    \"Merchant/Store Name\"]].rename(columns={ \"Partner Merchant ID\"  : \"CAB\",\n",
    "                                                                            \"Create Time\"           : \"DATETIME\",\n",
    "                                                                            \"Merchant Scope\"        : \"ID\",\n",
    "                                                                            \"Issuer Identifier\"     : \"NOM\",\n",
    "                                                                            \"Merchant/Store Name\"   : \"Status\"}).fillna(\"\")\n",
    "\n",
    "reduced_qris                =  reduced_qris.drop(reduced_qris[reduced_qris['CAB'] == ''].index)\n",
    "reduced_qris                =  reduced_qris.drop(reduced_qris[reduced_qris['Status'] == 'Withdrawal'].index)\n",
    "\n",
    "reduced_qris['DATETIME']    =   pd.to_datetime(reduced_qris['DATETIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "reduced_qris['DATE']        =   reduced_qris['DATETIME'].dt.strftime('%d/%m/%Y')\n",
    "reduced_qris['TIME']        =   reduced_qris['DATETIME'].dt.time\n",
    "\n",
    "reduced_qris['CODE']        =   \"\"\n",
    "reduced_qris['KAT']         =   \"QRIS SHOPEE\"\n",
    "reduced_qris['SOURCE']      =   \"INVOICE\"\n",
    "\n",
    "# re-order columns\n",
    "reduced_qris                =   reduced_qris[[\"CAB\", \"DATE\", \"TIME\", \"CODE\", \"ID\", \"NOM\", \"KAT\", \"SOURCE\"]]\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "reduced_qris.to_csv(os.path.join(final_path, 'reduced_QRIS.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<h1>PREPARING GOJEK 1 <i>(Laporan Transaksi GoFood)<i><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining Gojek Invoice Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Gojek1\n",
    "all_files = os.listdir(gojek1_path)\n",
    "\n",
    "# Filter out non-CSV files\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "# Create a list to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(gojek1_path, csv)\n",
    "    try:\n",
    "        # Try reading the file using default UTF-8 encoding\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Branch'] = csv.split(\".\")[0]\n",
    "        df_list.append(df)\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # If UTF-8 fails, try reading the file using UTF-16 encoding with tab separator\n",
    "            df = pd.read_csv(file_path, sep='\\t', encoding='utf-16')\n",
    "            df['Branch'] = csv\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {csv} because of error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = merge_path+\"/\"+'merged_GOJEK1.csv'\n",
    "\n",
    "try:\n",
    "    big_gojek1 = pd.concat(df_list, ignore_index=True)\n",
    "    if os.path.exists(new_file_path):\n",
    "        os.remove(new_file_path)\n",
    "        big_gojek1.to_csv(new_file_path, index=False)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning Combined Gojek \"<i>Laporan Transaksi...<i>\" File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gojek1 = pd.read_csv(merge_path+'\\merged_GOJEK1.csv',sep=',',decimal='.')\n",
    "\n",
    "reduced_gojek1   =   merged_gojek1.loc[:,[\"Waktu Selesai (WIB)\",\n",
    "                                        \"Nomor Pesanan\",\n",
    "                                        \"Harga Makanan\",\n",
    "                                        \"Branch\"]].rename(columns={ \"Waktu Selesai (WIB)\"   : \"DATETIME\",\n",
    "                                                                    \"Nomor Pesanan\"         : \"ID\",\n",
    "                                                                    \"Harga Makanan\"         : \"NOM\",\n",
    "                                                                    \"Branch\"                : \"CAB\"}).fillna(\"\")\n",
    "\n",
    "reduced_gojek1['DATETIME']    =   pd.to_datetime(reduced_gojek1['DATETIME'])\n",
    "reduced_gojek1['DATE']        =   reduced_gojek1['DATETIME'] .dt.strftime('%d/%m/%Y')\n",
    "reduced_gojek1['TIME']        =   reduced_gojek1['DATETIME'] .dt.time\n",
    "\n",
    "reduced_gojek1['NOM']         =   pd.to_numeric(reduced_gojek1['NOM']).astype(int)\n",
    "\n",
    "reduced_gojek1['CODE']        =   \"\"\n",
    "reduced_gojek1['KAT']         =   \"GO RESTO\"\n",
    "reduced_gojek1['SOURCE']      =   \"INVOICE\"\n",
    "\n",
    "# re-order columns\n",
    "reduced_gojek1                =   reduced_gojek1[[\"CAB\", \"DATE\", \"TIME\", \"CODE\", \"ID\", \"NOM\", \"KAT\", \"SOURCE\"]]\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "reduced_gojek1.to_csv(os.path.join(final_path, 'reduced_GOJEK1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<h1>PREPARING GOJEK 2 <i>(Mie_Gacoan_..._P...)<i><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Gojek2\n",
    "all_files = os.listdir(gojek2_path)\n",
    "\n",
    "# Filter out non-CSV files\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "# Create a list to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(gojek2_path, csv)\n",
    "    try:\n",
    "        # Try reading the file using default UTF-8 encoding\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Branch'] = csv.split(\".\")[0]\n",
    "        df_list.append(df)\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # If UTF-8 fails, try reading the file using UTF-16 encoding with tab separator\n",
    "            df = pd.read_csv(file_path, sep='\\t', encoding='utf-16')\n",
    "            df['Branch'] = csv\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {csv} because of error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "\n",
    "new_file_path = merge_path+\"/\"+'merged_GOJEK2.csv'\n",
    "\n",
    "try:\n",
    "    big_gojek2 = pd.concat(df_list, ignore_index=True)\n",
    "    if os.path.exists(new_file_path):\n",
    "        os.remove(new_file_path)\n",
    "        big_gojek2.to_csv(new_file_path, index=False)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gojek2 = pd.read_csv(merge_path+'\\merged_GOJEK2.csv',sep=',',decimal='.')\n",
    "merged_gojek2 = merged_gojek2.dropna(axis=1, how='all')\n",
    "\n",
    "reduced_gojek2   =   merged_gojek2.loc[:,[\"Transaction Time\",\n",
    "                                          \"Order\",\n",
    "                                          \"Amount\",\n",
    "                                          \"Branch\"]].rename(columns={ \"Transaction Time\": \"DATETIME\",\n",
    "                                                                      \"Order\"           : \"ID\",\n",
    "                                                                      \"Amount\"          : \"NOM\",\n",
    "                                                                      \"Branch\"          : \"CAB\"}).fillna(\"\")\n",
    "\n",
    "reduced_gojek2['DATETIME']    =   pd.to_datetime(reduced_gojek2['DATETIME'], format='%b %d %Y, %H:%M')\n",
    "reduced_gojek2['DATE']        =   reduced_gojek2['DATETIME'] .dt.strftime('%d/%m/%Y')\n",
    "reduced_gojek2['TIME']        =   reduced_gojek2['DATETIME'] .dt.time\n",
    "\n",
    "reduced_gojek2['NOM']         =   pd.to_numeric(reduced_gojek2['NOM']).astype(int)\n",
    "\n",
    "reduced_gojek2['CODE']        =   \"\"\n",
    "reduced_gojek2['KAT']         =   \"GO RESTO\"\n",
    "reduced_gojek2['SOURCE']      =   \"INVOICE\"\n",
    "\n",
    "# re-order columns\n",
    "reduced_gojek2                =   reduced_gojek2[[\"CAB\", \"DATE\", \"TIME\", \"CODE\", \"ID\", \"NOM\", \"KAT\", \"SOURCE\"]]\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "reduced_gojek2.to_csv(os.path.join(final_path, 'reduced_GOJEK2.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<h1>PREPARING WEB<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting html-styled .xls to .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_files = os.listdir(web_path)\n",
    "\n",
    "#convert html-styled xls to xlsx\n",
    "for file in xls_files:\n",
    "    if file.endswith(\".xls\"):\n",
    "        file_path = os.path.join(web_path, file)\n",
    "        \n",
    "        with open(file_path) as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "        tables = soup.find_all('table')\n",
    "        writer = pd.ExcelWriter(file_path.replace(\".xls\", \".xlsx\"), engine='openpyxl', mode='w')\n",
    "        \n",
    "        for i, table in enumerate(tables):\n",
    "            caption = table.find('caption')\n",
    "            if caption:\n",
    "                sheet_name = caption.get_text().strip()\n",
    "            else:\n",
    "                sheet_name = 'Sheet{}'.format(i+1)\n",
    "            df = pd.read_html(StringIO(str(table)))[0]\n",
    "            df.drop(1, inplace=True)\n",
    "            df.to_excel(sheet_name=sheet_name, index=False, header=False, excel_writer=writer,)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting .xlsx to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select xlsx files only\n",
    "xlsx_files = glob.glob(os.path.join(web_path, '*.xlsx'))\n",
    "\n",
    "#convert xlsx to csv\n",
    "for xlsx in xlsx_files:\n",
    "    try:\n",
    "        read_xlsx = pd.read_excel(xlsx)\n",
    "        file_name = xlsx[xlsx.find('WEB') + 4:-5]\n",
    "        read_xlsx.to_csv(os.path.join(web_path, file_name + '.csv'), index = None, header=True)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining all .csv of WEB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine WEB\n",
    "all_files = os.listdir(web_path)\n",
    "\n",
    "# Filter out non-CSV files\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "# Create a list to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(web_path, csv)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_web = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = merge_path+\"/\"+'merged_WEB.csv'\n",
    "\n",
    "if os.path.exists(new_file_path):\n",
    "    os.remove(new_file_path)\n",
    "\n",
    "big_web.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning combined web file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_web = pd.read_csv(merge_path+'\\merged_web.csv',sep=',',decimal='.')\n",
    "\n",
    "reduced_web                =  merged_web.loc[:,[\"DATE\",\n",
    "                                                \"CAB\",\n",
    "                                                \"CODE\",\n",
    "                                                \"TIME.1\",\n",
    "                                                \"KATEGORI\",\n",
    "                                                \"CUSTOMER\",\n",
    "                                                \"TOTAL\"]].rename(columns={\"TIME.1\"      : \"TIME\",\n",
    "                                                                        \"KATEGORI\"    : \"KAT\",\n",
    "                                                                        \"CUSTOMER\"    : \"ID\",\n",
    "                                                                        \"TOTAL\"       : \"NOM\"}).fillna(\"\")\n",
    "\n",
    "reduced_web                =  reduced_web.drop(reduced_web[reduced_web['DATE'] == 'TOTAL'].index)\n",
    "\n",
    "reduced_web['DATE']        =   pd.to_datetime(reduced_web['DATE'], format='%Y-%m-%d')\n",
    "reduced_web['DATE']        =   reduced_web['DATE'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "reduced_web['NOM']         =   pd.to_numeric(reduced_web['NOM']).astype(int)\n",
    "\n",
    "reduced_web['SOURCE']      =   \"WEB\"\n",
    "\n",
    "# re-order columns\n",
    "reduced_web                =   reduced_web[[\"CAB\", \"DATE\", \"TIME\", \"CODE\", \"ID\", \"NOM\", \"KAT\", \"SOURCE\"]]\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "reduced_web.to_csv(os.path.join(final_path, 'reduced_web.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "<h1>COMBINE ALL FINAL FILES <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine ALL\n",
    "all_files = os.listdir(final_path)\n",
    "\n",
    "# Filter out non-CSV files\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "# Create a list to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    file_path = os.path.join(final_path, csv)\n",
    "    try:\n",
    "        # Try reading the file using default UTF-8 encoding\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_ready = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the final result to a new CSV file\n",
    "new_file_path = script_dir+\"/\"+'Merge-Ready.xlsx'\n",
    "\n",
    "if os.path.exists(new_file_path):\n",
    "    os.remove(new_file_path)\n",
    "\n",
    "big_ready.to_excel(new_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
